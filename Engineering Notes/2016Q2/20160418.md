# Framework
* Populate etcd with configs and Framework selects the right config.
* We set how many nsm there should be.
* Documentation needs to be there! Would like to have end of the month
    * Deepak and Narayan need to learn also. Kannan needs to join also.
* Scalability : vdisks - limit -> FS interface is still slow. Still need to validate with 5000 vdisks, 360 disks.
    * To be tested on OVH
* Multiple ASDs on 1 disk still pending.
* Accelerated is already merged in
* Geoscale setup to be tested across 3 datacenters in OVH (France)
* Fragment cache needs to be local. Extend will have an issue if you need to scale across datacenters. You can override it manually. We need to show datacenters in the GUI (MAY).
* No need for upgrade code.

# ALBA
* Integration of the IOexecuter file (
    * dedicated thread for handling IO, if enough  dispatch IO (like setting iodepth). Wait for x u secs (configurable) or x (8 or so) requests (use separate timer thread).
        * Fetch 400 usecs
    * IO submit  in user thread from the kernel: performance is slower but lower latency.
        * No accumulation of Io before dispatched
    * 2 ssds - 100.000 files of 4MB each  on the ASDs with filesystem (xfs) no page cache
    * Maybe IO submit version is best.
* RDMA integration still working on it.
* 1 ASD = 1 ocaml thread - Performance 60K iops
* Sandeep  has tested with his code 80K iops per thread.
* Datacenter awareness - > we didnâ€™t pick it up yet (next month)
* We should test arakoon across the wire.
* Latency on intel ssds for reads (without network and rocksdb)? 90 usecs seems a bit too high
* IOexecuter block: bypass the filesystem and talks to block direct. Metadata is done. No garbage collection yet.  Block compared to filesystem fast 70% faster and lower cpu for reads. Writes are similar. Has full write persistence. Garbage collector will be optimized for flash.
* Ocaml doing the write path. The io execute used for reads? To be discussed.
